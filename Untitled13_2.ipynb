{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовим данные. Поехали!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_data = pd.read_csv('docs_titles.tsv', sep='\\t', encoding='utf-8', lineterminator='\\n')\n",
    "train_groups = pd.read_csv('train_groups.csv')\n",
    "test_groups = pd.read_csv('test_groups.csv')\n",
    "sample_subm = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28026\n"
     ]
    }
   ],
   "source": [
    "doc_to_title = {}\n",
    "with open('docs_titles.tsv', encoding='utf-8') as f:\n",
    "    for num_line, line in enumerate(f):\n",
    "        if num_line == 0:\n",
    "            continue\n",
    "        data = line.strip().split('\\t', 1)\n",
    "        doc_id = int(data[0])\n",
    "        if len(data) == 1:\n",
    "            title = ''\n",
    "        else:\n",
    "            title = data[1]\n",
    "        doc_to_title[doc_id] = title\n",
    "print (len(doc_to_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train_groups.csv')\n",
    "traingroups_titledata = {}\n",
    "for i in range(len(train_data)):\n",
    "    new_doc = train_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    target = new_doc['target']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in traingroups_titledata:\n",
    "        traingroups_titledata[doc_group] = []\n",
    "    traingroups_titledata[doc_group].append((doc_id, title, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для очистки стоп слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', ':', ';', '-', '?', '@', '!', '(', ')']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "#my_stop = [',', '.', ':', ';', '-', '?', '@', '!', '(',')', '—', '|', '}','{','[',']', '/','\\'', 'и', 'а','на','за','в', 'под']\n",
    "my_stop = [',', '.', ':', ';', '-', '?', '@', '!', '(',')']\n",
    "print(my_stop)\n",
    "for i in my_stop:\n",
    "    stop_words.add(i)\n",
    "\n",
    "def delete_stop_words(title):\n",
    "    word_tokens = set(word_tokenize(title))\n",
    "    filtered_sentence = set()\n",
    "    for w in word_tokens:\n",
    "        if w  not in stop_words:\n",
    "            filtered_sentence.add(w)\n",
    "    res = filtered_sentence\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовим признаки на ТРЭЙН"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [31:56<00:00, 14.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 20) (11690,) (11690,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "X_train = []\n",
    "groups_train = []\n",
    "lems_words = []\n",
    "lems_words_j = []\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for new_group in tqdm(traingroups_titledata):\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        groups_train.append(new_group)\n",
    "        all_dist = []\n",
    "        words = delete_stop_words(title)\n",
    "        lems_words = []\n",
    "        for z in words:\n",
    "            p = morph.parse(z)[0]\n",
    "            lems_words.append(p.normal_form)\n",
    "        words = set(lems_words)\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j, target_j = docs[j]\n",
    "            words_j = delete_stop_words(title_j)\n",
    "            lems_words_j = []\n",
    "            for z_j in words_j:\n",
    "                p = morph.parse(z_j)[0]\n",
    "                lems_words_j.append(p.normal_form)\n",
    "            words_j = set(lems_words_j)\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_train.append(sorted(all_dist, reverse=True)[0:20])\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Признаки by Sanya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:18<00:00,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 20) (11690,) (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "X_train_ = []\n",
    "groups_train = []\n",
    "lems_words = []\n",
    "lems_words_j = []\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for new_group in tqdm(traingroups_titledata):\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    text = []\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        words = delete_stop_words(title)\n",
    "        lems_words = []\n",
    "        for z in words:\n",
    "            p = morph.parse(z)[0]\n",
    "            lems_words.append(p.normal_form)\n",
    "        title1 = ''\n",
    "        for i in lems_words:\n",
    "            title1 += i\n",
    "            title1 += ' '\n",
    "        text.append(title1)\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (1,2))\n",
    "    vectorizer.fit(text)\n",
    "    vector = vectorizer.transform(text).toarray()\n",
    "    for i in range(len(text)):\n",
    "        res_vector = vector[i]\n",
    "        res_vector = np.sort(res_vector)\n",
    "        X_train_.append(res_vector[-20:])\n",
    "X_train_ = np.array(X_train_)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train_.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.29811126, 0.29811126,\n",
       "        0.32475821],\n",
       "       [0.1901183 , 0.1901183 , 0.1901183 , ..., 0.20711221, 0.20711221,\n",
       "        0.20711221],\n",
       "       [0.19204005, 0.20455407, 0.20920573, ..., 0.20920573, 0.20920573,\n",
       "        0.20920573],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.57735027, 0.57735027,\n",
       "        0.57735027],\n",
       "       [0.13006314, 0.19956497, 0.19956497, ..., 0.21785876, 0.21785876,\n",
       "        0.39912995],\n",
       "       [0.18388873, 0.18388873, 0.18388873, ..., 0.18388873, 0.18388873,\n",
       "        0.18388873]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 129/129 [00:25<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11690, 15) (11690,) (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_train = []\n",
    "X_train_1 = []\n",
    "groups_train = []\n",
    "lems_words = []\n",
    "lems_words_j = []\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for new_group in tqdm(traingroups_titledata):\n",
    "    docs = traingroups_titledata[new_group]\n",
    "    text = []\n",
    "    for k, (doc_id, title, target_id) in enumerate(docs):\n",
    "        y_train.append(target_id)\n",
    "        words = delete_stop_words(title)\n",
    "        lems_words = []\n",
    "        for z in words:\n",
    "            p = morph.parse(z)[0]\n",
    "            lems_words.append(p.normal_form)\n",
    "        title1 = ''\n",
    "        for i in lems_words:\n",
    "            title1 += i\n",
    "            title1 += ' '\n",
    "        text.append(title1)\n",
    "    vectorizer1 = CountVectorizer()\n",
    "    vectorizer1.fit(text)\n",
    "    vector = vectorizer1.transform(text).toarray()\n",
    "    cosinus = cosine_similarity(vector)\n",
    "    for i in range(len(text)):\n",
    "        cosinus_i = cosinus[i]\n",
    "        res = np.sort(cosinus_i)\n",
    "        X_train_1.append(res[-15:])\n",
    "X_train_1 = np.array(X_train_1)\n",
    "y_train = np.array(y_train)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_train_1.shape, y_train.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 15)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_max = np.hstack((X_train,X_train_,X_train_1)) ##объединяем признаки мои и сани"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11690, 55)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_max.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВАЛИДАЦИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(X_max, y_train, test_size=0.35, shuffle=False)#, random_state=0, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.001, 4000, 15, 0.7517899761336516)\n",
      "(0.001, 4000, 16, 0.7532881626145874)\n",
      "(0.001, 4000, 17, 0.7520891364902507)\n",
      "(0.001, 4000, 18, 0.7513944223107568)\n",
      "(0.001, 4000, 19, 0.7531847133757962)\n",
      "(0.001, 4000, 20, 0.7511961722488039)\n",
      "(0.001, 4000, 21, 0.7524910322837783)\n",
      "(0.001, 4000, 25, 0.7524910322837783)\n",
      "(0.001, 4500, 15, 0.7518856689162365)\n",
      "(0.001, 4500, 16, 0.753071739992073)\n",
      "(0.001, 4500, 17, 0.7538644470868014)\n",
      "(0.001, 4500, 18, 0.753071739992073)\n",
      "(0.001, 4500, 19, 0.7526796347757047)\n",
      "(0.001, 4500, 20, 0.7526796347757047)\n",
      "(0.001, 4500, 21, 0.7515873015873015)\n",
      "(0.001, 4500, 25, 0.7520825069416899)\n",
      "(0.001, 5000, 15, 0.7526796347757047)\n",
      "(0.001, 5000, 16, 0.7540594059405941)\n",
      "(0.001, 5000, 17, 0.7518796992481203)\n",
      "(0.001, 5000, 18, 0.7512850929221035)\n",
      "(0.001, 5000, 19, 0.7511885895404121)\n",
      "(0.001, 5000, 20, 0.7510882469331223)\n",
      "(0.001, 5000, 21, 0.7503961965134707)\n",
      "(0.001, 5000, 25, 0.7516831683168316)\n",
      "(0.001, 5500, 15, 0.7524752475247525)\n",
      "(0.001, 5500, 16, 0.7542540561931144)\n",
      "(0.001, 5500, 17, 0.7521773555027712)\n",
      "(0.001, 5500, 18, 0.7524752475247525)\n",
      "(0.001, 5500, 19, 0.7512850929221035)\n",
      "(0.001, 5500, 20, 0.75)\n",
      "(0.001, 5500, 21, 0.7507911392405063)\n",
      "(0.001, 5500, 25, 0.7523734177215189)\n",
      "(0.001, 6000, 15, 0.7497034400948992)\n",
      "(0.001, 6000, 16, 0.7531746031746032)\n",
      "(0.001, 6000, 17, 0.7510882469331223)\n",
      "(0.001, 6000, 18, 0.7505938242280286)\n",
      "(0.001, 6000, 19, 0.7498020585906572)\n",
      "(0.001, 6000, 20, 0.7484126984126984)\n",
      "(0.001, 6000, 21, 0.7510882469331223)\n",
      "(0.001, 6000, 25, 0.7516831683168316)\n",
      "(0.001, 6500, 15, 0.7496038034865293)\n",
      "(0.001, 6500, 16, 0.751486325802616)\n",
      "(0.001, 6500, 17, 0.7493069306930693)\n",
      "(0.001, 6500, 18, 0.7505938242280286)\n",
      "(0.001, 6500, 19, 0.7481158270527569)\n",
      "(0.001, 6500, 20, 0.7486122125297383)\n",
      "(0.001, 6500, 21, 0.7491082045184304)\n",
      "(0.001, 6500, 25, 0.7515822784810127)\n",
      "(0.001, 7000, 15, 0.7511885895404121)\n",
      "(0.001, 7000, 16, 0.7499009116131591)\n",
      "(0.001, 7000, 17, 0.7500990099009901)\n",
      "(0.001, 7000, 18, 0.7506936187078874)\n",
      "(0.001, 7000, 19, 0.7488114104595879)\n",
      "(0.001, 7000, 20, 0.7492063492063493)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [18:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-2941a69006df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m19\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m21\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msubsample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.75\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    888\u001b[0m                     \u001b[0mvalid_sets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_le\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m         super().fit(X, _y, sample_weight=sample_weight, init_score=init_score, eval_set=valid_sets,\n\u001b[0m\u001b[0;32m    891\u001b[0m                     \u001b[0meval_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_sample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                     \u001b[0meval_class_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_init_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_init_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m    681\u001b[0m             \u001b[0minit_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbooster_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 683\u001b[1;33m         self._Booster = train(params, train_set,\n\u001b[0m\u001b[0;32m    684\u001b[0m                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    247\u001b[0m                                     evaluation_result_list=None))\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   2641\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__set_objective_to_none\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2642\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot update due to null objective function.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2643\u001b[1;33m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0m\u001b[0;32m   2644\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2645\u001b[0m                 ctypes.byref(is_finished)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reg = []\n",
    "for learning_rate in tqdm([0.001,0.002]): #, 0.3, 0.4, 0.5):\n",
    "    for n_estimators in([4000,4500,5000,5500,6000,6500,7000,7500,8000]):\n",
    "        for max_depth in (15,16,17,18,19,20,21,25):\n",
    "            clf = LGBMClassifier(learning_rate=learning_rate, n_estimators=n_estimators, max_depth=max_depth,subsample=0.75)\n",
    "            clf.fit(X_train_val, y_train_val)\n",
    "            y_pred = clf.predict(X_test_val)\n",
    "            f1 = f1_score(y_test_val, y_pred)\n",
    "            reg.append((learning_rate, n_estimators, max_depth, f1))\n",
    "            print((learning_rate, n_estimators, max_depth, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.002, 4000, 25, 0.7547619047619047)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = 0\n",
    "for i in reg:\n",
    "    if i[3]>max:\n",
    "        max=i[3]\n",
    "        max_elem = i\n",
    "max_elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_5 = []\n",
    "depth_7 = []\n",
    "depth_10 = []\n",
    "depth_13 = []\n",
    "depth_15 = []\n",
    "depth_21 = []\n",
    "depth_18 = []\n",
    "for i in reg:\n",
    "    if i[2]==18:\n",
    "        depth_18.append(i[3])\n",
    "    if i[2]==21:\n",
    "        depth_21.append(i[3])\n",
    "    if i[2]==25:\n",
    "        depth_25.append(i[3])\n",
    "    if i[2]==15:\n",
    "        depth_15.append(i[3])\n",
    "    if i[2]==13:\n",
    "        depth_13.append(i[3])\n",
    "    if i[2]==10:\n",
    "        depth_10.append(i[3])\n",
    "    if i[2]==7:\n",
    "        depth_7.append(i[3])\n",
    "    if i[2]==5:\n",
    "        depth_5.append(i[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_groups.csv')\n",
    "testgroups_titledata = {}\n",
    "for i in range(len(test_data)):\n",
    "    new_doc = test_data.iloc[i]\n",
    "    doc_group = new_doc['group_id']\n",
    "    doc_id = new_doc['doc_id']\n",
    "    title = doc_to_title[doc_id]\n",
    "    if doc_group not in testgroups_titledata:\n",
    "        testgroups_titledata[doc_group] = []\n",
    "    testgroups_titledata[doc_group].append((doc_id, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 180/180 [51:24<00:00, 17.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 20) (16627,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "X_test = []\n",
    "groups_test = []\n",
    "lems_words = []\n",
    "lems_words_j = []\n",
    "for new_group in tqdm(testgroups_titledata):\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        groups_test.append(new_group)\n",
    "        all_dist = []\n",
    "        words = delete_stop_words(title)\n",
    "        lems_words = []\n",
    "        for z in words:\n",
    "            p = morph.parse(z)[0]\n",
    "            lems_words.append(p.normal_form)\n",
    "        words = set(lems_words)\n",
    "        for j in range(0, len(docs)):\n",
    "            if k == j:\n",
    "                continue\n",
    "            doc_id_j, title_j = docs[j]\n",
    "            words_j = delete_stop_words(title_j)\n",
    "            lems_words_j = []\n",
    "            for z_j in words_j:\n",
    "                p = morph.parse(z_j)[0]\n",
    "                lems_words_j.append(p.normal_form)\n",
    "            words_j = set(lems_words_j)\n",
    "            all_dist.append(len(words.intersection(words_j)))\n",
    "        X_test.append(sorted(all_dist, reverse=True)[0:20])\n",
    "X_test = np.array(X_test)\n",
    "groups_test = np.array(groups_test)\n",
    "print (X_test.shape,groups_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 180/180 [00:31<00:00,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 20) (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_ = []\n",
    "groups_test = []\n",
    "lems_words = []\n",
    "lems_words_j = []\n",
    "for new_group in tqdm(testgroups_titledata):\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    text = []\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        words = delete_stop_words(title)\n",
    "        lems_words = []\n",
    "        for z in words:\n",
    "            p = morph.parse(z)[0]\n",
    "            lems_words.append(p.normal_form)\n",
    "        title1 = ''\n",
    "        for i in lems_words:\n",
    "            title1 += i\n",
    "            title1 += ' '\n",
    "        text.append(title1)\n",
    "    vectorizer = TfidfVectorizer(ngram_range = (1,3))\n",
    "    vectorizer.fit(text)\n",
    "    vector = vectorizer.transform(text).toarray()\n",
    "    for i in range(len(text)):\n",
    "        res_vector = vector[i]\n",
    "        res_vector = np.sort(res_vector)\n",
    "        X_test_.append(res_vector[-20:])\n",
    "X_test_ = np.array(X_test_)\n",
    "groups_test = np.array(groups_test)\n",
    "print (X_test_.shape,groups_test.shape)\n",
    "#дохуя ошибок все исправить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 180/180 [00:48<00:00,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16627, 15) (0,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_1 = []\n",
    "groups_train = []\n",
    "lems_words = []\n",
    "lems_words_j = []\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for new_group in tqdm(testgroups_titledata):\n",
    "    docs = testgroups_titledata[new_group]\n",
    "    text = []\n",
    "    for k, (doc_id, title) in enumerate(docs):\n",
    "        words = delete_stop_words(title)\n",
    "        lems_words = []\n",
    "        for z in words:\n",
    "            p = morph.parse(z)[0]\n",
    "            lems_words.append(p.normal_form)\n",
    "        title1 = ''\n",
    "        for i in lems_words:\n",
    "            title1 += i\n",
    "            title1 += ' '\n",
    "        text.append(title1)\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(text)\n",
    "    vector = vectorizer.transform(text).toarray()\n",
    "    cosinus = cosine_similarity(vector)\n",
    "    for i in range(len(text)):\n",
    "        cosinus_i = cosinus[i]\n",
    "        res = np.sort(cosinus_i)\n",
    "        X_test_1.append(res[-15:])\n",
    "X_test_1 = np.array(X_test_1)\n",
    "groups_train = np.array(groups_train)\n",
    "print (X_test_1.shape, groups_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627, 55)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_max_test = np.hstack((X_test,X_test_,X_test_1))\n",
    "X_max_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем модель с лучшими параметрами и погнали предиктить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.002, max_depth=25, n_estimators=4000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LGBMClassifier(learning_rate=0.002, n_estimators=4000, max_depth=25)\n",
    "clf.fit(X_max, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = clf.predict(X_max_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm.target = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16627,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_subm.to_csv('submit5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
